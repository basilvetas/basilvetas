# Deep Learning and CNNs

For a long time the term Deep Learning remained somewhat of a black box for me.  So over the past couple of months I've spent a bit of time studying and trying to better understand the core concepts and machine learning models that underpin what is commonly referred to as Deep Learning.  Primarily, these are Artifical Neural Networks, also called Multilayer Perceptrons as well as Convolutional Neural Networks which are currently the state-of-the-art for most tasks related to computer vision and image recognition. This post is intended as a primer on understanding the basics of Deep Learning and clarifying the key differences between Convolutional Neural Networks and the more general Multilayer Perceptron/Neural Network architecture.

### Deep Learning

What does it mean when people say Deep Learning? In practice, Deep Learning is a class of machine learning algorithms that use multiple layers of processing in which each successive layer uses the output from the previous layer as input, until finally providing classification values at the end.  The key distinguishing feature of a Deep Learning model is the multiple "hidden" layers in between the input and output layers. In most cases, the architecture for Deep Learning systems is inspired by the structure of neurons in the brain, where activation signals in one neuron are successively passed through synapses to other neurons causing them to fire off activation signals of their own. This architecture is evident in the canoncial example of the Multilayer Perceptron. But before getting into that, I will spend a brief moment discussing its precursor, the Single Layer Perceptron.

### Single Layer Perceptron

The first building block for understanding Artificial Neural Networks, and ultimately CNNs is the Single Layer Perceptron. The Perceptron is an algorithm for learning a linear classifier, namely, it learns the weights **W** of a function that maps a real-valued input vector **X** to an output value using the Sign function, called the activation function, for binary classification. In the image below, each input node is a component of the input vector **X**, which gets computed in the dot product with the weight vector **W** and added to the bias term **b**. This result is passed to the Sign function to get the output classification. The weights are learned using a training data set with which we iteratively make classification attempts and evaluate them against the true classification values based on a 0-1 Loss function. During training, when incorrect classifcations are made, the weights are updated accordingly. Prediction is then done by simply computing the dot product and Sign function on any set of future input values.

![Single Layer Perceptron](images/perceptron.png)

### Artificial Neural Networks

With a general understanding of how the Single Layer Perceptron works, it is easy to extend this model to a Multilayer Perceptron model, often just called a Neural Network. In the case of the Perceptron, we just had a single layer consisting of a single node that we passed our input values into after taking the dot product with weights. Now we have many layers, each with many nodes and corresponding weights that we pass our input values into. Our inputs, which I simply treat as the vector **X** in the image below, are still taken as a dot product with the weigh vector **W** and added to bias term **b** before being passed into an activation function in each respective node. However, in the neural network this process repeats through each layer in a feed-forward manner and eventually generates probability scores in the output layer, typically using a softmax function, that are evaluated against a loss function, usually L2 loss.

A key feature to point out here is that the Mutlilayer Perceptron model is fully-connected, meaning that each node in a layer shares a connection with all nodes in the immediately successive and previous layers, however nodes within a layer share no connections among themselves. This results in increased memory and computational complexity as the number of nodes within layers are increased.

![Artificial Neural Network](images/neural-net.png)

Now to discuss some of the key differences between this Neural Network architecture and the Single Layer Perceptron. First, recall in the latter case we used the Sign function as the activation function within our node, because this has the nice property of outputting either a 0 or a 1 as a final classification value for each training data example. With the Neural Network, this property is now problematic because the Sign function is not differentiable, and the training stage of the model uses a method called backpropogation in which we apply [Gradient Decent](https://en.wikipedia.org/wiki/Gradient_descent) to iteratively update the weights in each layer based on the gradient of the loss function. So instead, for the Neural Network it is common to use a continuous Sigmoid function that creates an S-curve, sort of approximating the Sign function. The most frequently use Sigmoids are either the logistic function seen in the image, or the hyperbolic tangent. The Neural Network can then be trained on some dataset through multiple Epochs: iterations of the feed-forward calculations using Sigmoid activations, calculation of the error or loss, and followed by backpropogation to update the weights in each layer based on the error gradient and learning rate, allowing us to learn the weights over time. The training algorithm terminates once the error reaches some predefined threshold.

Lastly, I want to summarize the key parameters and hyperparameters required in a Multilayer Perceptron implementation, which further down will help reveal some of the key differences between this architecture and that of Convolutional Neural Networks.

Parameters | Hyperparameters | Weights | Activation function | Bias | Learning rate | Error threshold | Nodes per layer | Loss function

### Convolutional Neural Networks

Now that we've seen the general Neural Network architecture, I will touch on Convolutional Neural Networks, aka ConvNets or CNNs, which have received a lot of attention since around 2011 when a significant performance breakthrough was made using CNNs for image recognition. The reason the architecture for CNNs is so different from the Multilayer Perceptron is because they make the key assumption that the input values are images, which allows for some key optimizations that take advantage of the spatial properties of an image. Namely, instead of having as input a one-dimensional vector of nodes, we now have as input a three-dimensional image consisting of the width, height and depth (in this case depth contains the color values, typically length 3 for the RGB color encoding). Additionally, rather than identical hidden layers throughout the model, the CNN has a few distinct hidden layer that perform different transformations: the Convolutional Layer, Pooling Layer, and Fully-Connected Layer for the most basic architectures. In the image below, only a single Convolutional Layer, Pooling Layer and Fully-Connected Layer is shown, but in practice there may be multiple instances of all three (note however, that a Pooling Layer always comes after a Convolutional Layer, and Fully-Connected Layers always show up at the end).

![Convolutional Neural Network](images/cnn.png)

In the Convolutional Layer, the spatial structure now allows for the implementation of Locally-Connected Filters with Shared Weights. As you can see in the image, now nodes within each layer are no longer fully-connected to previous and subsequent layers, but only locally-connected along the width and height based on some predefined filter (the blue rectangle). These filters are essentially our weights that map a receptive field (the green rectangle) to the output value (red rectangle) by again calculating the dot product, adding bias, and passing it through an activation function to generate a single activation map. Shown in the image is just a single filter, that will slide across the image to calcuate the corresponding output value for each region in the original image, however, in reality many filters will be used to scan over the image, each generating a unique activation map, and these are layered together in what is called the output volume (notice in the image the ouput from the convolutional layer is three-dimensional).

Important to note, that each filter has a unique set of shared weights that ultimately will learn to recognize specific elements within an image (such as edges, shapes, or even faces), but while the filters are only locally-connected across the width and height of the image, they are fully-connected in the depth dimension. Another important distinction is that since we are dealing with locally-connected regions, the activation function we use is applied elementwise after taking the dot product within the local region. Researchers have found that a new activation function called a Rectified Linear Unit (ReLU) performs quite well for CNNs and is the most commonly used activation function as of the past few years. Sometimes the application of the activation function is considered a layer in-itself, referred to as the ReLU Layer.

After the Convolutional Layer, a Pooling Layer is used as a downsampling mechanism to control capactiy, computational complexity and overfitting in the network. In a similar manner to the Convolutional Layer, a filter is used that will slide across each activation map and apply a function to the corresponding receptive field. There are different functions that can be used for pooling, such as Average Pooling that takes the average value from the receptive field, or most commonly Max Pooling as shown in the image, which takes the maximum value in each receptive field. The output of the Max Pooling Layer is simply a scaled-down image along the width and height, while depth remains the same (note that for the original image, the depth represented the RGB values, but after the first Convolution, the depth in the remaining layers corresponds to the number of filters used that are stacked together to form the activation volume).

I mentioned briefly before that the filters are "predefined", which begs the question of how to actually define or create these filters in practice. This is done with a few hyperparamters: filter size (number of pixels, such as a 5x5x3 frame), stride (number of pixels we shift as we slide the filter, typically 1 or 2, occasionally more), and zero-padding (only for the Convolutional Layers, basically 0s around the border of the input image for convenience). Lastly, a final hyperparameter is the actual number of filters used in each layer, which as I said before will correspond to the depth of the output volume.

The final layer is the Fully-Connected layer, which functions exactly like the Multilayer Perceptron and allows us to convert a final input volume to class scores often using the softmax function. Otherwise, while the math is a bit more involved than with regular Neural Networks, CNNs are still trained using backpropogation based on computing the Error for some Loss Function and applying Gradient Descent to iteratively update the weight values of each filter until an error threshold is reached. For comparison, I again summarize the parameter and hyperparameters commonly used in the Convolutional Neural Network architecture for comparison to general Multilayer Perceptrons.

Parameters | Convolutional Hyperparameters | Pooling Hyperparameters | Fully-Connected/general Hyperparameters | Weights | Activation function | Pooling function | Learning rate | Bias | Filter size | Error threshold | Stride | Loss function | Number of filters | Zero Padding
